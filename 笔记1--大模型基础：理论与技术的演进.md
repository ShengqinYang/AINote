## 第一章：大模型基础--理论与技术的演进


#### RNN
#### 注意力机制：Attention
#### Transform
#### Bert
#### Gpt



#### 一些问答：
##### 1. 每次向 GPT 提问，回答注意力的权重都是不一样的吗？
```text
答：不一样，GPT自注意力模型，GPT在接受问题不同时，计算注意力权重的机制是不同的，回答时的注意力权重也不同。你问chatgpt相同问题，它不一定给你相同的回答。
```
##### 2. Transformer 中，如果 Encoder 和 Decoder 都是 Self-Attention ，那么如何处理问题和答案之间的关系？（通过 Mask 吗？）
```text
答：问题与答案之间的信息泄露？ 是通过mask技术，掩码让问题不能全部看到答案，答案不能全部看到问题，避免信息泄露。
```
##### 3. Encoder-Decoder Attention 是 Self-Attention 吗？
```text
答：不算是，这个今晚彭老师着重讲过。encoder-decoder attention从编码器中能关注到特定的位置，self-attention是模型内部的自注意力机制，可以关注到输入序列中的所有位置。
```
##### 4. Bert是基于编码器，GPT是基于解码器，不是编码和解码一块用吗？
```text
答：这3句话都没错，2个维度。 Bert和GPT的训练都用到了transformer，都用transformer的编码器和解码器来做预训练。但， Bert目的是捕捉上下文信息，它强化了编码器能力。GPT的目的是序列生成能力，它用自回归强化了解码器能力。
```
##### 5. 到底应该怎么理解”参数“，几十亿的参数是什么概念，理解成收音机的旋转按钮？跟编程语言的函数参数可以类比吗？ 对应到Transformer中的什么地方
```text
答：简单点说，参数就是你可以调节的变量。 学术点说，参数是模型训练中可调节的变量和权重，函数中的权重参数，tansformer中的权重和偏置矩阵。 训练的目的，就是找到最佳的参数值。
```
##### 6. 我讲讲个人朴素的理解，讲的不对请老师和大家指正，信息传递总是会受损的，编码解码过程也会受损，一个人要把自己大脑中的信息讲出来，要编码，信息就会受损，听到的人再在自己的大脑中解码也会受损，但是同对于一个事情，大家大脑中的思考是差不多的
```text
答：基本认同，LLM就是在理解语言本身，也即是表示（representation）学习的范畴。大语言模型初步解决了：跨语言同语义问题，同语言长距离（指代）问题等。
```

##### 7.当前gpt3.5的语义识别和bert哪个强，同样的语料（比如几万条）微调bert和gpt3.5的ada预训练模型哪个效果好？
```text
答：这两个侧重点不同，你关注在上下文信息或语义理解上，就选Bert；关注在任务生成上，就选GPT。侧重点不同。

问：比如企业中后台做一些运维操作，对产品的一些自然语言输入需要解析出操作的对象meta信息，然后作为参数调用api后，解析response最终做一个summary返回给用户。
是不是关注原始输入解析到meta调用api这一步，bert理论上会更有优势？
但实践中看，langchain调用gpt3.5利用 outputparser效果也不错，调整prompt后没看到多少解析不对的情况，然后应该可以调用agent调完api，结合问题，转到gpt优势领域生成回答了
所以是不是可以认为除了一些垂直领域如特定情感分析场景，力大砖飞的GPT可以把所有前辈扫到垃圾桶了？

答：是这样的，关注原始输入解析到meta信息 理论上bert具有优势 Bert可以学习到更加丰富的上下文信息和语义信息， Bert是有优势的。
但实际应用比较多样，垂直领域的应用场景具有特定的语言特征和结构，需要更精细的模型进行调整和优化，针对这种特定领域，langchain调用gpt3.5的精调，可能更具有优势。
不过，对于一些垂直领域，尤其是垂直的狭窄的有很多标注数据的，GPT大语言模型不一定就比其他有针对优化的模型效果更好。
还是要具体场景具体分析，不过对于应用场景，建议可以把longchain的GPT3.5当成baseline，现在用起来成本不高，而且这个baseline起点不低。
```

##### 8.我要做文本分类，text1已经通过llm找到了分类label1了，我将（text1，label1）缓存起来。我想输入侧的text2不经过llm和text1对比拿label，有什么方式吗？
```text
答：没太听懂，tex1->label1通过llm做了标注，然后text2不想通过llm了。 那是变成了text2 和tex1的关系来推label2?那你需要挖text2 和text1的关系，特征工程比如词袋模型？然后用传统的SVM机器学习？
```
##### 9.向量的比较有余弦相似度等等，这些是不需要经过llm的吧？现在市面上最主流的向量相似度比较方法是什么？
```text
答：嗯，不是所有的都需要经过llm的，虽然llm也可以解决。 余弦？欧几里得？曼哈顿？字符串的编辑距离？常用的是这些，应该还有其他的一些。
```




- 同学间互相传话（RNNs）
- 老师查小纸条（解码器Decoder）
- 老师叫来每个同学问话（注意力机制Attention）
- 你画我猜（Multi-head Attention）
- 向左看齐（BERT）
