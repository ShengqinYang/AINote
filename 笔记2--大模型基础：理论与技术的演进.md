## 第一章：大模型基础--理论与技术的演进（第二节课）
### 第一部分：课后习题
<hr />

### 第二部分：相关名词解释
<hr />

### 第三部分：一些问答

##### 1、SFT fine-tuning、fine-turning 和 Instruction-Tuning的区别？
```text
答：Fine-tuning：微调
SFT(Supervised Fine-Tuning)：有监督的微调
Instruction-Tuning：指令微调
1.Fine-tuning: 这是一种通用的深度学习训练策略，通常在预训练模型基础上进行，以适应新任务。通过对预训练模型的参数进行微调，使模型能够在特定任务上表现更好。
2.SFT: 这是一个监督学习过程，即通过人工标注的训练数据集来调整模型的参数。在这个过程中，模型学习将输入映射到预期的输出。SFT就是一种Fine-tuning的策路。
3.Instruction-Tuning: 是OpenAI提出的一种新的训练方法。通过向模型提供人类编写的指导性指令，对模型进行训练和调整。主要用于增强模型对特定指令的响应。
这些指令可以是对输入输出的期望行为描述，如"重要性：高；可读性：中；重要性：低"，从而控制模型在生成文本时更好地满足用户需求。使模型能够正确理解并生成预期的输出。

Prompt和Instruction都是用于指导模型生成输出的文本，不对模型的权重进行修改； fine tunning会对模型的权重进行修改。 Prompt和Instruction比较相似，关注的目标不一样。
```
<br />

##### 2、我要用大模型做分类，我用openai的finetune，效果很好，6w条语料4轮epoch可以到0.94accuracy。但是高质量的语料标注是个难活。我朋友告诉我，prompt-tuning用的语料更少，而语料治疗要求没那么高，而效果更好。
```text
文本分类是一个经典下游任务，如果你的预料本身并没有给使用的OpenAI基础模型增加额外的知识，那么prompt-tuning确实是更有性价比的
关于prompt-based learning目的是让大家理解如何从 standard prompt 演进到适用性极高的 ToT 的。
```
<br />

##### 3、思维链，是指提示词提问时的思维链，还是模型回答问题时运用思维链来回答？我的理解是后者，因为昨天的后半程课，全程讲的是模型在回答问题时的思维链逻辑推理，但是助教直播间说的是提示词提问时用思维链方式。能捋一捋吗？昨天并没有讲什么样的提示词规则更高效

```text
对于一个模型在回答问题时的思维链逻辑推理，可以将其理解为模型对输入问题进行处理和分析的过程。这个过程可以看作是一种思维链，其中模型通过触发不同的关联或推理机制来生成答案。
当使用提示词提问时，思维链方式可以指引你以一种递进的思考方式来构建问题。你可以从一个提示词出发，将其作为起点，然后依次深入探索相关的概念、关联或细节，构建一个连贯的问题链条。每个问题都基于前一个问题的答案或信息，从而逐步扩展和深化对问题的理解。
通过思维链方式提问可以帮助你更系统地探索和获取信息，从而得到更准确、全面的回答。这种方法可以用于讨论复杂的主题、解决问题或促进深入思考和探索。
```
<br />

##### 4、请教一个小小问题 我看观看视频时 发现如图中的关于Encoder-decoder Architecture的公式和图示 虽然是引自原论文 但我认为高亮部分的公式和Sample是有差异的：
1. 公式中：hi是encoder的hidden state, cj是context vector为了计算出decoder的hidden state（有点类似hi是输入cj是输出）, 所以attention weights即aij的角标关系是先i后j, 即先hi输入后cj输出
2. 但Sample公式中：c2 = a21·h1 + a22·h2 + a23·h3 其attention weights即aij的角标关系是先j后i, 其实与公式正好相反, 我觉得这个Sample的算式表达是有问题的
![avatar](resource/笔记2-3-4.1.png)

```text
答：要理解这个问题呢，有几个关键第一个就是这个角标你提的很好，l和J在这个表格里面呢，其实我们能看到1呢，其实是指输入我们的encounter编码器的隐藏状态。Hi这个1表示的是输入的一个特定位置，J是输出，就是我们的这个 dcoder，我们的解码器想要去定位的那个位置，就那个特定的词，所以I通常指的是我们图当中的们的比如说 S2Y2 这样的一个输出这个是没问题的，然后在表格里面呢，其实我们能看到它的逻辑是顺畅的，就是我们用 CJ。就是表达我们输出的地接一个隐藏层的状态它是由这个矩阵乘上每一个输入乘出来的的和。
至于在那个图例里面，为什么感觉好像它的角标倒过来了啊，我的理解啊，个人的理解，作者这里呢更多的是想通过这个图例来表达含义。但是在这里呢，其实就。某种层面上把这个阿尔法这个矩阵，他做了一个转制啊，做这个转制也是为了这个图做的更好看，因为你看这个图中呢，其实中间那一部分是一个神经网络，然后这个神经网络的输出呢，是一个S。然后呢，它的这个每一每一行是一个 S，每一列呢，上面又变成了一个H，这个顺序呢，就跟我们其实正常认为的应该输入是H，输出是 S，有点倒过来了，所以它是为了这个图例的表达方便。我理解在图例当中，把这个阿尔法矩阵做了一个转制，对就相当于它的行和列做了一次交换
但就理解公式而言，你从表格来看是没有问题的，就表格可能会更明确一点，不管是S还是 C，其实都表达的是解码层的一个特定位置的隐藏成状态，或者它对应的上下文向量，那么对应的这个位置这个J就比如说我们这里看到的 S2 或者C2，都是说锚定了一个输出的位置，然后把这个。腾讯 which，就是我们的注意力权重和对应的输入的这个hi去做相乘，然后再做这个西格玛，就是把它求和，这个逻辑是没问题的。核心就是这里我理解是为了图的表达形式上更直观，做了一次转制的一个展示啊，这我个人的理解。
```
```text 
回复：好的 明白了 非常感谢彭老师的细致解答 我一开始也是感觉把矩阵转置了 但没明白为什么要转置 你说NN剪头对应输出所以横向是输出S 这么说的话确实有道理 我还特意重画了一下 ￼
```
![avatar](resource/笔记2-3-4.2.png)

<br />

##### 5、有监督学习和无监督学习，远程监督是啥意思
```text
答：有监督，就是你需要标注信息，在给机器学习。 无监督就是你直接可以给，不需要进行标注。 
远程监督是一种半监督学习方法，它的基本思想是利用已有的知识库自动生成标注数据。在远程监督中，模型会利用知识库中的实体和关系信息，自动标注相关的文本数据，
然后使用这些标注数据训练模型。相对于有监督学习，远程监督可以减少标注数据的工作量，但标注数据的质量受到知识库的影响。
```
<br />

##### 6、GPT-4相较GPT-3、GPT-3.5的优势
```text
答：
    1. 多模态模型: GPT-4支持图像输入，出色的视觉信息理解能力使得GPT-4能对接更多样化的下游任务， 
        如:描述不寻常图像中的幽默、总结截屏文本以及回答包含图表的试题。在文本理解能力上，GPT-4 在中 文和多轮对话中也表现出远超 GPT-3.5 的能力。
    2. 扩展上下文窗口:gpt-4 and gpt-4-32k 分别提供了最大长度为8192和32768个token的上下文窗口。
        这使得 GPT-4可以通过更多的上下文来完成更复杂的任务，也为 思维链(CoT)、思维树(ToT)等后续工作提供 了可能。
    3. GPT+生态 :借助GPT-4强大能力，依托 ChatGPT Plugin 搭建AIGC应用生态商店(类似 App Store)
    4. 应用+GPT :GPT-4已经被应用在多个领域，包括微软Office、Duolingo、Khan Academy等。
```
<br />

##### 7、Prompt Turing(即：Prompt Enginner) vs Prompt learning vs In-context Learning
```text
Prompt Learning是一种使用预训练语言模型的方法，它不会修改模型的权重。关注的是如何通过设计 `有效的提示` 来 `引导` 模型的输出。
In-context Learning是一种使用预训练语言模型的方法，它不会修改模型的权重。关注的是如何 `利用` 输入序列中的 `上下文信息` 来影响模型的输出。

prompt learning更关注于如何 `训练` 模型以响应特定的提示；
prompt tuning则更关注于如何 `找到最优的提示` 以提高模型的性能。
```

小点：相同梯子也可以换站点，换了站点一般ip就换了。关闭chrome, 换站点, 换完后电脑重启一下，我遇到后大部分是这三步同时进行，印象中是100%解决。
