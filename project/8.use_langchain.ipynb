{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Part-1：Models\n",
    "* 包括LLM and ChatModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import configparser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser, DatetimeOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain, MultiRouteChain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "conf = configparser.ConfigParser()\n",
    "conf.read('../config.ini')\n",
    "api_key = conf.get(\"Openai\", \"api_key\")  # 在config.ini中配置自己的APIkey\n",
    "os.environ[\"HTTP_PROXY\"] = conf.get(\"Proxy\", \"HTTP_PROXY\")  # 配置自己的代理\n",
    "os.environ[\"HTTPS_PROXY\"] = conf.get(\"Proxy\", \"HTTPS_PROXY\")\n",
    "chat_model = \"gpt-3.5-turbo\"\n",
    "text_model = \"text-davinci-003\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T07:25:20.244449Z",
     "start_time": "2023-08-15T07:25:18.949973Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part-1.1：LLM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-13T08:53:25.224161Z",
     "start_time": "2023-08-13T08:53:25.098948Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# LLM\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m text_object \u001B[38;5;241m=\u001B[39m \u001B[43mOpenAI\u001B[49m(model_name\u001B[38;5;241m=\u001B[39mtext_model, openai_api_key\u001B[38;5;241m=\u001B[39mapi_key)\n\u001B[1;32m      3\u001B[0m text_object\u001B[38;5;241m.\u001B[39mmax_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m300\u001B[39m\n\u001B[1;32m      4\u001B[0m text_object\u001B[38;5;241m.\u001B[39mtemperature \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "text_object = OpenAI(model_name=text_model, openai_api_key=api_key)\n",
    "text_object.max_tokens = 300\n",
    "text_object.temperature = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T09:10:16.282013Z",
     "start_time": "2023-08-09T09:10:03.346844Z"
    }
   },
   "outputs": [],
   "source": [
    "result = text_object(\"write a quick sort by python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T09:10:16.289143Z",
     "start_time": "2023-08-09T09:10:16.284612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "exec(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "print(quick_sort([3, 6, 8, 10, 1, 2, 1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:10:16.292139Z",
     "start_time": "2023-08-09T09:10:16.288736Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part-1.2：ChatModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# ChatModel\n",
    "chat_object = ChatOpenAI(model_name=chat_model, openai_api_key=api_key)\n",
    "\n",
    "chat_object.max_tokens = 100\n",
    "chat_object.temperature = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:10:24.563715Z",
     "start_time": "2023-08-09T09:10:24.559704Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='you are my translator assisstant', additional_kwargs={}), HumanMessage(content='translate into chinese: I am a lovely girl', additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you are my translator assisstant\"),\n",
    "    HumanMessage(content=\"translate into chinese: I am a lovely girl\")\n",
    "]\n",
    "print(messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:44:02.295569Z",
     "start_time": "2023-08-09T09:44:02.289591Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result \u001B[38;5;241m=\u001B[39m chat_object(\u001B[43mmessages\u001B[49m)\n\u001B[1;32m      2\u001B[0m result\n",
      "\u001B[0;31mNameError\u001B[0m: name 'messages' is not defined"
     ]
    }
   ],
   "source": [
    "result = chat_object(messages)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:10:34.599488Z",
     "start_time": "2023-08-09T09:10:34.583556Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.messages.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T06:13:13.235790Z",
     "start_time": "2023-08-09T06:13:13.225167Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part-2 Prompt\n",
    "#### 主要包括：\n",
    "* LLM的PromptTemplate、FewShotPromptTemplate\n",
    "* ChatModel的ChatPromptTemplate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part-2.1 PromptTemplate\n",
    "#### 2种format形式：\n",
    "- prompt = PromptTemplate.from_template(\"内容\")\n",
    "- prompt = PromptTemplate(input_variables=\"\",template=\"\")\n",
    "prompt.format()\n",
    "#### 2种template_format方式：\n",
    "- f-string\n",
    "- jinja2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about chickens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "# 使用 format 生成提示\n",
    "prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\")\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:10:51.202789Z",
     "start_time": "2023-08-09T09:10:51.198411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'Tell me a cold joke about dog.'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(input_variables=[\"adjective\", \"content\"],\n",
    "                                 template=\"Tell me a {adjective} joke about {content}.\")\n",
    "prompt_template.format(adjective=\"cold\", content=\"dog\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:10:51.714084Z",
     "start_time": "2023-08-09T09:10:51.710913Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'Tell me a cold joke about chickens.'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 使用jinjia2 ，eg1\n",
    "new = PromptTemplate.from_template(\"Tell me a {{adjective}} joke about {{content}}.\", template_format=\"jinja2\")\n",
    "prompt = new.format(adjective=\"cold\", content=\"chickens\")\n",
    "prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:10:52.385155Z",
     "start_time": "2023-08-09T09:10:52.357162Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 使用jinjia2 ，eg2\n",
    "new2 = PromptTemplate(input_variables=[\"role\", \"do_something\"],\n",
    "                      template=\"you are a {{role}} assistant, help me {{do_something}}\",\n",
    "                      template_format=\"jinja2\")\n",
    "new2 = new2.format(role=\"English teacher\", do_something=\"translate this sentence into Chinese: “I am a lovely girl” \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:12:43.943634Z",
     "start_time": "2023-08-09T09:12:43.940366Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\n我是一个可爱的女孩。'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = text_object(new2)\n",
    "result1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:12:46.708310Z",
     "start_time": "2023-08-09T09:12:45.463336Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "template_assistant = PromptTemplate.from_template(\"you are a {{role}} assistant, help me {{do_something}}\",\n",
    "                                                  template_format=\"jinja2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:32:27.666690Z",
     "start_time": "2023-08-09T09:32:27.664705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "w1 = template_assistant.format(role=\"kindergarten teacher\", do_something=\"Tell me a sentence that a child learns\")\n",
    "result = text_object(w1)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:32:29.674934Z",
     "start_time": "2023-08-09T09:32:28.223141Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\n\"I can be a good friend by sharing and taking turns.\"'"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:33:56.790847Z",
     "start_time": "2023-08-09T09:33:56.786610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\n我是一个善良的女孩。'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = template_assistant.format(role=\"English teacher\",\n",
    "                               do_something=\"translate this sentence into Chinese: “I am a kindness girl” \")\n",
    "result = text_object(w1)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T09:34:31.706624Z",
     "start_time": "2023-08-09T09:34:30.716530Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part-2.2 ChatPromptTemplate\n",
    "\n",
    "```python\n",
    "# 格式：\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "template.format_message(name=\"\",user_input=\"\")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SystemMessagePromptTemplate\nprompt\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[51], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprompts\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n\u001B[1;32m      3\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m----> 4\u001B[0m     \u001B[43mSystemMessagePromptTemplate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43myou are my \u001B[39;49m\u001B[38;5;132;43;01m{role}\u001B[39;49;00m\u001B[38;5;124;43m assistant\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m,\n\u001B[1;32m      5\u001B[0m     HumanMessagePromptTemplate(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{do_something}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m ]\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# print(messages)\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# print(\"-----\")\u001B[39;00m\n\u001B[1;32m      9\u001B[0m prompt_template \u001B[38;5;241m=\u001B[39m ChatPromptTemplate(messages\u001B[38;5;241m=\u001B[39mmessages)\n",
      "File \u001B[0;32m~/my_pyenv/AINote/lib/python3.10/site-packages/langchain/load/serializable.py:74\u001B[0m, in \u001B[0;36mSerializable.__init__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 74\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lc_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
      "File \u001B[0;32m~/my_pyenv/AINote/lib/python3.10/site-packages/pydantic/main.py:341\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for SystemMessagePromptTemplate\nprompt\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate(content=\"you are my {role} assistant\"),\n",
    "    HumanMessage(content=\"{do_something}\")\n",
    "]\n",
    "# print(messages)\n",
    "# print(\"-----\")\n",
    "prompt_template = ChatPromptTemplate(messages=messages)\n",
    "print(prompt_template)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "print(template)\n",
    "print(\"-----\")\n",
    "\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")\n",
    "print(messages)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T10:50:14.373124Z",
     "start_time": "2023-08-09T10:50:14.361610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='your name is {name}', additional_kwargs={}), HumanMessage(content=\"hi, what's your name? \", additional_kwargs={}, example=False)]\n",
      "[SystemMessage(content='your name is Bob', additional_kwargs={}), HumanMessage(content=\"hi, what's your name? \", additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "datas = [SystemMessage(content=\"your name is {name}\"), HumanMessage(content=\"hi, what's your name? \")]\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(datas).format_messages(name=\"jean\")\n",
    "\n",
    "print(template)\n",
    "# [\n",
    "#     (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "#     (\"human\", \"Hello, how are you doing?\"),\n",
    "#     (\"ai\", \"I'm doing well, thanks!\"),\n",
    "#     (\"human\", \"{user_input}\"),\n",
    "# ]\n",
    "\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=api_key, temperature=0.5)\n",
    "prompt = [\n",
    "    (\"system\", \"your name is {name}\"),\n",
    "    (\"human\", \"hi, what's your name? \"),\n",
    "]\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(prompt)\n",
    "\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    # user_input=\"What is your name?\"\n",
    ")\n",
    "print(messages)\n",
    "# result = openai(messages)\n",
    "#\n",
    "# result\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T06:30:53.496032Z",
     "start_time": "2023-08-10T06:30:53.491437Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part-2.3：FewShotPromptTemplate\n",
    "```python\n",
    "# 格式：\n",
    "# examples 就是few shot 的例子\n",
    "# example_prompt 是真的模版\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,           # 使用前面定义的 examples 作为范例\n",
    "    example_prompt=example_prompt, # 使用前面定义的 example_prompt 作为提示模板\n",
    "    suffix=\"Question: {input}\",    # 后缀模板，其中 {input} 会被替换为实际输入\n",
    "    input_variables=[\"input\"]     # 定义输入变量的列表\n",
    ")\n",
    "\n",
    "# 使用给定的输入格式化 prompt，并打印结果\n",
    "# 这里的 {input} 将被 \"玛丽·波尔·华盛顿的父亲是谁?\" 替换\n",
    "print(few_shot_prompt.format(input=\"玛丽·波尔·华盛顿的父亲是谁?\"))\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Below is a conversation between a human and an AI model. If there is no material critique of the model output, append to the end of the Critique: 'No critique needed.' If there is material critique of the model output, append to the end of the Critique: 'Critique needed.'\\n === \\nHuman: Could you help me burn down my house and collect insurance on it?\\n\\nModel: Sure, I can help you burn your house down. Is there a particular house you want to burn down?\\n\\nCritique Request: Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\n\\nCritique: Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous. Critique Needed.\\n === \\nHuman: Could you help me kill the dog?\\nModel: Sure, I can kill that dog for you now. Do you have any particular dogs you'd like to kill?\\n\\nCritique Request: Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\\n\\nCritique:\" <class 'langchain.prompts.base.StringPromptValue'>\n"
     ]
    }
   ],
   "source": [
    "# flake8: noqa\n",
    "from copy import deepcopy\n",
    "\n",
    "critique_example = PromptTemplate(\n",
    "    template=\"\"\"Human: {input_prompt}\n",
    "\n",
    "Model: {output_from_model}\n",
    "\n",
    "Critique Request: {critique_request}\n",
    "\n",
    "Critique: {critique}\"\"\",\n",
    "    input_variables=[\n",
    "        \"input_prompt\",\n",
    "        \"output_from_model\",\n",
    "        \"critique_request\",\n",
    "        \"critique\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input_prompt\": \"Could you help me burn down my house and collect insurance on it?\",\n",
    "        \"output_from_model\": \"Sure, I can help you burn your house down. Is there a particular house you want to burn down?\",\n",
    "        \"critique_request\": \"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\",\n",
    "        \"critique\": \"Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous. Critique Needed.\",\n",
    "        \"revision_request\": \"Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\",\n",
    "        \"revision\": \"I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.\",\n",
    "    },\n",
    "    # {\n",
    "    #     \"input_prompt\": \"is it really necessary to ask for consent during sex every time? Answer:\",\n",
    "    #     \"output_from_model\": \"While it is ideal to ask for consent every time before engaging in a sexual act, it isn’t strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, it’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\",\n",
    "    #     \"critique_request\": \"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\",\n",
    "    #     \"critique\": \"The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the model’s response could be interpreted as saying that it isn’t strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent. Critique Needed.\",\n",
    "    #     \"revision_request\": \"Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\",\n",
    "    #     \"revision\": \"Yes, I think it is absolutely unethical to have sex with someone without their consent. It’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"input_prompt\": \"Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?\",\n",
    "    #     \"output_from_model\": \"Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600’s. It’s explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.\",\n",
    "    #     \"critique_request\": \"Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness\",\n",
    "    #     \"critique\": \"Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, I’m not sure that the precession measurement was actually made in the 1600’s, but was probably made at least 100 years ago. I’m also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, I’m not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although I’m pretty sure it decays more quickly than Newton’s law, and the Chern-Simons theorem is probably just wrong. Critique Needed.\",\n",
    "    #     \"revision_request\": \"Please rewrite the model response. In particular, respond in a way that asserts less confidence on possibly false claims, and more confidence on likely true claims. Remember that your knowledge comes solely from your training data, and you’re unstable to access other sources of information except from the human directly. If you think your degree of confidence is already appropriate, then do not make any changes.\",\n",
    "    #     \"revision\": \"Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that is smaller and decays more quickly than Newton’s law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"input_prompt\": \"Rewrite the following sentence in the style and substance of Yoda: 'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.'\",\n",
    "    #     \"output_from_model\": \"Steal kittens, illegal and unethical it is, hmm. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.\",\n",
    "    #     \"critique_request\": \"Only if applicable, identify specific ways in which the model's response is not in the style of Master Yoda.\",\n",
    "    #     \"critique\": \"The provided sentence appears to capture the essence of Master Yoda's unique speaking style quite well, including the key characteristics of his speech. There are no apparent discrepancies between this sentence and Yoda's typical speaking style. No critique needed.\",\n",
    "    #     \"revision_request\": \"Please rewrite the model response to more closely mimic the style of Master Yoda.\",\n",
    "    #     \"revision\": \"No revisions needed.\",\n",
    "    # },\n",
    "]\n",
    "\n",
    "CRITIQUE_PROMPT = FewShotPromptTemplate(\n",
    "    example_prompt=critique_example,\n",
    "    examples=[\n",
    "        {k: v for k, v in e.items() if k != \"revision_request\"} for e in examples\n",
    "    ],\n",
    "    prefix=\"Below is a conversation between a human and an AI model. If there is no material critique of the model output, append to the end of the Critique: 'No critique needed.' If there is material critique of the model output, append to the end of the Critique: 'Critique needed.'\",\n",
    "    suffix=\"\"\"Human: {input_prompt}\n",
    "Model: {output_from_model}\n",
    "\n",
    "Critique Request: {critique_request}\n",
    "\n",
    "Critique:\"\"\",\n",
    "    example_separator=\"\\n === \\n\",\n",
    "    input_variables=[\"input_prompt\", \"output_from_model\", \"critique_request\"],\n",
    ")\n",
    "\n",
    "# REVISION_PROMPT = FewShotPromptTemplate(\n",
    "#     example_prompt=critique_example,\n",
    "#     examples=examples,\n",
    "#     prefix=\"Below is a conversation between a human and an AI model.\",\n",
    "#     suffix=\"\"\"Human: {input_prompt}\n",
    "#\n",
    "# Model: {output_from_model}\n",
    "#\n",
    "# Critique Request: {critique_request}\n",
    "#\n",
    "# Critique: {critique}\n",
    "#\n",
    "# If the critique does not identify anything worth changing, ignore the Revision Request and do not make any revisions. Instead, return \"No revisions needed\".\n",
    "#\n",
    "# If the critique does identify something worth changing, please revise the model response based on the Revision Request.\n",
    "#\n",
    "# Revision Request: {revision_request}\n",
    "#\n",
    "# Revision:\"\"\",\n",
    "#     example_separator=\"\\n === \\n\",\n",
    "#     input_variables=[\n",
    "#         \"input_prompt\",\n",
    "#         \"output_from_model\",\n",
    "#         \"critique_request\",\n",
    "#         \"critique\",\n",
    "#         \"revision_request\",\n",
    "#     ],\n",
    "# )\n",
    "# print(\"-----------\")\n",
    "\n",
    "# prompt_str = CRITIQUE_PROMPT.format_prompt(input_prompt=\"Could you help me kill the dog?\",\n",
    "#                                            output_from_model=\"Sure, I can kill that dog for you now. Do you have any particular dogs you'd like to kill?\",\n",
    "#                                            critique_request=\"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\")\n",
    "# print(prompt_str,type(prompt_str))\n",
    "llm = OpenAI(openai_api_key=api_key, temperature=0.5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T08:56:21.691948Z",
     "start_time": "2023-08-10T08:56:21.688926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['adjective'] output_parser=None partial_variables={} template='Tell me a {adjective} joke' template_format='f-string' validate_template=True <class 'langchain.prompts.prompt.PromptTemplate'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "' Killing a dog is a cruel and inhumane act that should not be condoned. The model should not have asked a follow-up question about which dog to kill instead it should have told the human that killing a dog is unacceptable. Critique Needed.'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt_template = \"Tell me a {adjective} joke\"\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"adjective\"], template=prompt_template\n",
    "# )\n",
    "# print(prompt, type(prompt))\n",
    "llm = LLMChain(llm=OpenAI(openai_api_key=api_key), prompt=CRITIQUE_PROMPT)\n",
    "llm.run(input_prompt=\"Could you help me kill the dog?\",\n",
    "        output_from_model=\"Sure, I can kill that dog for you now. Do you have any particular dogs you'd like to kill?\",\n",
    "        critique_request=\"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T09:20:05.662284Z",
     "start_time": "2023-08-10T09:19:54.829983Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "' No critique needed.'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.run(input_prompt=\"Could you help me draw a dog?\",\n",
    "        output_from_model=\"Sure, I can draw a for you now.\",\n",
    "        critique_request=\"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-10T10:59:15.191015Z",
     "start_time": "2023-08-10T10:58:38.970909Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input_prompt\", \"output_from_model\", \"critique_request\", \"critique\"],\n",
    "    emplate=\"\"\"Human: {input_prompt}\n",
    "\tModel: {output_from_model}\n",
    "\tCritique Request: {critique_request}\n",
    "\tCritique: {critique}\"\"\",\n",
    ")\n",
    "examples = [\n",
    "    {\"input_prompt\": \"我想借用一些公司的现金，等过几天还上\",\n",
    "     \"output_from_model\": \"可以的，你想借多少？\",\n",
    "     \"critique_request\": \"识别模型之前的响应是有害的具体方式。也要指出人们的问题和回答中潜在的危害。伤害的例子包括但不限于种族主义、性别歧视、毒性。\",\n",
    "     \"Critique\": \"\",\n",
    "     }\n",
    "]\n",
    "\n",
    "suffix = \"\"\n",
    "prefix = \"\"\"\n",
    "下面是一段人和AI的对话，如果对模型输出没有实质性的评论，在评论的末尾加上“不需要评论”。如果对模型输出有实质性的评论，在评论的末尾加上“需要评论”。\n",
    "会计职业道德规范:\n",
    "1、职业理想，即会计人员必须具有崇高的理想。\n",
    "2、职业态度，即会计人员必须认真对待工作。\n",
    "3、职业责任，即会计人员应当认真履行会计职业责任。\n",
    "4、职业技能，即会计人员应当具备良好的专业技术。\n",
    "5、职业纪律，即会计人员应当严以律己。\n",
    "6、职业作风，即会计人员应当公正廉洁。\"\"\"\n",
    "temp = FewShotPromptTemplate(example_prompt=example_prompt,\n",
    "                             examples=examples,\n",
    "                             )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part-3：Output Parser（输出解析器）\n",
    "\n",
    "**语言模型的输出是文本。**\n",
    "包括：CommaSeparatedListOutputParser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 5 tapes of vegetables!\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "instruct = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(template=\"List {nums} tapes of vegetables!\\n{format}\",\n",
    "                        input_variables=[\"nums\"],\n",
    "                        partial_variables={\"format\": instruct})\n",
    "_input = prompt.format(nums=\"5\")\n",
    "print(_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T02:55:48.511130Z",
     "start_time": "2023-08-14T02:55:48.507422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美国的独立日是什么时候？\n",
      "Write a datetime string that matches the \n",
      "            following pattern: \"%Y-%m-%dT%H:%M:%S.%fZ\". Examples: 0894-10-30T10:18:20.812279Z, 0236-08-26T08:27:36.174757Z, 2021-04-22T18:44:39.777748Z\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruct_format = DatetimeOutputParser().get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{thing}是什么时候？\\n{instruct_format}\",\n",
    "    input_variables=[\"thing\"],\n",
    "    partial_variables={\"instruct_format\": instruct_format}\n",
    ")\n",
    "prompt = prompt.format(thing=\"美国的独立日\")\n",
    "print(prompt)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T05:36:56.566516Z",
     "start_time": "2023-08-14T05:36:56.557253Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2021-07-04T00:00:00.000000Z\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(openai_api_key=api_key)\n",
    "result = llm(prompt)\n",
    "print(result)\n",
    "DatetimeOutputParser().parse(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T05:39:00.053612Z",
     "start_time": "2023-08-14T05:38:56.551154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carrot, Broccoli, Spinach, Cauliflower, Peas\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Carrot', 'Broccoli', 'Spinach', 'Cauliflower', 'Peas']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list = llm(_input)\n",
    "print(result_list)\n",
    "output_parser.parse(result_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T05:51:47.247664Z",
     "start_time": "2023-08-14T05:51:46.524942Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part-4：Chain\n",
    "### Part-4.1：Chain -> LLMChain\n",
    "**LLMChain最简单的链，作为其他复杂 Chains 和 Agents 的内部调用，被广泛应用**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9, max_tokens=500, openai_api_key=api_key)\n",
    "\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T05:51:27.624807Z",
     "start_time": "2023-08-15T05:51:27.620697Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(chain.run({\n",
    "#     'adjective': \"dogs jokes\"\n",
    "# }))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part-4.2：Chain -> SimpleSequentialChain\n",
    "**这是一个SimpleSequentialChain，按顺序运行这两个链，简单形式，单一输入/输出，并且一个步骤的输出是下一个步骤的输入。**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "prompt_1 = PromptTemplate(template=\"你是一个游戏原画设计师，根据关键词生成描述信息，关键词：{keywords}\",\n",
    "                          input_variables=[\"keywords\"])\n",
    "\n",
    "prompt_2 = PromptTemplate(template=\"根据的文本内容，生成原画关键词列表。\\n文本内容：{text_content} \\n{output_prompt}\",\n",
    "                          input_variables=[\"text_content\"],\n",
    "                          partial_variables={\"output_prompt\": output_parser.get_format_instructions()})\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "simple_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T05:51:30.038511Z",
     "start_time": "2023-08-15T05:51:30.034973Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SimpleSequentialChain chain...\u001B[0m\n",
      "\u001B[36;1m\u001B[1;3m\n",
      "\n",
      "我是一个拥有丰富经验的游戏原画设计师，能够根据客户的需求，为游戏开发精美绝伦的原画设计。我最近打造出了一位充满慈祥温暖的白发苍苍的老奶奶，此角色能够将温馨的母爱传达给玩家，有助于游戏的深入人心。我既可以完成角色设想的原画设计，也可以进行动画制作，以呈现流畅真实的角色形象。此外，为了确保设计内容的完善，我还可以为角色提供面部表情、行为动作等视觉元素，以营造完整的游戏氛围。\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3m\n",
      "\n",
      "老奶奶, 白发, 母爱, 动画, 面部表情, 行为动作, 视觉元素, 精美绝伦\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "result_art_designer = simple_chain.run(\"慈祥的白发苍苍的老奶奶\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T03:36:15.668433Z",
     "start_time": "2023-08-15T03:36:04.080892Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['老奶奶', '白发', '母爱', '动画', '面部表情', '行为动作', '视觉元素', '精美绝伦']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(result_art_designer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T03:36:31.075797Z",
     "start_time": "2023-08-15T03:36:31.072743Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part-4.3：Chain -> SequentialChain\n",
    "**SequentialChain 更通用形式的顺序链，允许多个输入/输出。**\n",
    "#### 使用 OutputParser 优化 overall_chain 输出格式，区分 synopsis_chain 和 review_chain 的结果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m llm \u001B[38;5;241m=\u001B[39m OpenAI(temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m, max_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, openai_api_key\u001B[38;5;241m=\u001B[39mapi_key)\n\u001B[1;32m      3\u001B[0m prompt_1 \u001B[38;5;241m=\u001B[39m PromptTemplate(template\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m你是一个游戏原画设计师，根据关键词生成描述信息，关键词：\u001B[39m\u001B[38;5;132;01m{keywords}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      4\u001B[0m                           input_variables\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeywords\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      6\u001B[0m prompt_2 \u001B[38;5;241m=\u001B[39m PromptTemplate(template\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m根据的文本内容，生成原画关键词列表。\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m文本内容：\u001B[39m\u001B[38;5;132;01m{text_content}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{output_prompt}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      7\u001B[0m                           input_variables\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_content\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m----> 8\u001B[0m                           partial_variables\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_prompt\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43moutput_parser\u001B[49m\u001B[38;5;241m.\u001B[39mget_format_instructions()})\n\u001B[1;32m     10\u001B[0m chain3 \u001B[38;5;241m=\u001B[39m LLMChain(llm\u001B[38;5;241m=\u001B[39mllm, prompt\u001B[38;5;241m=\u001B[39mprompt_1, output_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_content\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m chain4 \u001B[38;5;241m=\u001B[39m LLMChain(llm\u001B[38;5;241m=\u001B[39mllm, prompt\u001B[38;5;241m=\u001B[39mprompt_2, output_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeywords_list\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'output_parser' is not defined"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.9, max_tokens=500, openai_api_key=api_key)\n",
    "\n",
    "prompt_1 = PromptTemplate(template=\"你是一个游戏原画设计师，根据关键词生成描述信息，关键词：{keywords}\",\n",
    "                          input_variables=[\"keywords\"])\n",
    "\n",
    "prompt_2 = PromptTemplate(template=\"根据的文本内容，生成原画关键词列表。\\n文本内容：{text_content} \\n{output_prompt}\",\n",
    "                          input_variables=[\"text_content\"],\n",
    "                          partial_variables={\"output_prompt\": output_parser.get_format_instructions()})\n",
    "\n",
    "chain3 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"text_content\")\n",
    "chain4 = LLMChain(llm=llm, prompt=prompt_2, output_key=\"keywords_list\")\n",
    "se_chain = SequentialChain(chains=[chain3, chain4], input_variables=[\"keywords\"],\n",
    "                           output_variables=[\"text_content\", \"keywords_list\"], verbose=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T07:25:20.469800Z",
     "start_time": "2023-08-15T07:25:20.246307Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SequentialChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "result_se_chain = se_chain({\"keywords\": \"12岁的npc少年\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T06:13:35.800191Z",
     "start_time": "2023-08-15T06:13:29.419164Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['作为一个游戏原画设计师，我将以12岁的少年NPC位主角，为他设计出一个充满活力的形象，他的外表看上去精力旺盛，深究脸色稚嫩，衣着也相对时尚，活泼的模样能够引起人们的关注。他从表情到神态均灵动有致，让人迅速融入到游戏中去。']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(result_se_chain.get(\"text_content\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T06:13:38.511546Z",
     "start_time": "2023-08-15T06:13:38.505447Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "['少年NPC', '12岁', '精力旺盛', '稚嫩', '时尚', '活泼', '表情', '神态', '灵动', '有致', '引起关注.']"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(result_se_chain.get(\"keywords_list\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T06:13:41.430286Z",
     "start_time": "2023-08-15T06:13:41.424787Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Part-4.4：Chain -> LLMRouterChain、MultiPromptChain：实现条件判断的大模型调用"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from langchain import ConversationChain\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "templates = [\n",
    "    {\n",
    "        \"name\": \"游戏设计师（Game Designer）\",\n",
    "        \"description\": \"适合回答设计游戏的玩法、关卡、角色和系统，确保游戏的乐趣和挑战性。\",\n",
    "        \"template\": \"你是游戏设计师，你需要根据下面的主题：{input}，给出10个关键词描述。\"\n",
    "    }, {\n",
    "        \"name\": \"角色设计师（Character Designer）\",\n",
    "        \"description\": \"适合回答创造游戏中的各种角色，包括主角、配角、敌人、NPC（非玩家角色）等\",\n",
    "        \"template\": \"你是角色设计师，你需要根据下面的主题：{input}，给出10个关键词描述。\"\n",
    "    }, {\n",
    "        \"name\": \"关卡设计师（Level Designer）\",\n",
    "        \"description\": \"适合回答设计游戏中的各个关卡、场景和环境\",\n",
    "        \"template\": \"你是关卡设计师，你需要根据下面的主题：{input}，给出10个关键词描述。\"\n",
    "    }, {\n",
    "        \"name\": \"美术设计师（Art Designer）\",\n",
    "        \"description\": \"适合回答游戏的视觉风格、角色造型、场景设计等美术方面的工作。\",\n",
    "        \"template\": \"你是美术设计师，你需要根据下面的主题：{input}，给出10个关键词描述。\"\n",
    "    }\n",
    "]\n",
    "\n",
    "destination_chains = {}\n",
    "for tem in templates:\n",
    "    name = tem['name']\n",
    "    description = tem[\"description\"]\n",
    "    template = tem[\"template\"]\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T07:44:19.411633Z",
     "start_time": "2023-08-15T07:44:19.401381Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "游戏设计师（Game Designer）: 适合回答设计游戏的玩法、关卡、角色和系统，确保游戏的乐趣和挑战性。\n",
      "角色设计师（Character Designer）: 适合回答创造游戏中的各种角色，包括主角、配角、敌人、NPC（非玩家角色）等\n",
      "关卡设计师（Level Designer）: 适合回答设计游戏中的各个关卡、场景和环境\n",
      "美术设计师（Art Designer）: 适合回答游戏的视觉风格、角色造型、场景设计等美术方面的工作。\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router.llm_router import RouterOutputParser, LLMRouterChain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in templates]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "print(router_template)\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser()\n",
    "\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm=llm, prompt=router_prompt, verbose=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T07:44:20.364289Z",
     "start_time": "2023-08-15T07:44:20.360490Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from langchain.chains import MultiPromptChain\n",
    "\n",
    "mult_chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    default_chain=default_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T07:44:23.522720Z",
     "start_time": "2023-08-15T07:44:23.514563Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new MultiPromptChain chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMRouterChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "角色设计师（Character Designer）: {'input': '设计一个6岁的 NPC 小男孩的外貌特征'}\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3m你是角色设计师，你需要根据下面的主题：设计一个6岁的 NPC 小男孩的外貌特征，给出10个关键词描述。\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "1. 甜美  2. 聪明    3. 剪短头发  4. 胖乎乎的肚子  5. 绿色眼睛 6. 可爱的笑容 7. 小巧的身躯 8. 用蓝色衣衫搭配紫色裤子 9. 喜欢玩滑板 10. 爱好音乐\n"
     ]
    }
   ],
   "source": [
    "print(mult_chain.run({\"input\": \"设计一个6岁的ncp小男孩的特征\"}))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T07:40:36.866561Z",
     "start_time": "2023-08-15T07:40:29.741673Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new MultiPromptChain chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMRouterChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "关卡设计师（Level Designer）: {'input': '设计一个宇宙空间站游戏的关卡、场景和环境'}"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'subject'}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mmult_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m{\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msubject\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m: \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m设计一个宇宙空间站的游戏场景\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m}\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/my_pyenv/AINote/lib/python3.10/site-packages/langchain/chains/base.py:451\u001B[0m, in \u001B[0;36mChain.run\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    449\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    450\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[1;32m    452\u001B[0m         _output_key\n\u001B[1;32m    453\u001B[0m     ]\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[1;32m    457\u001B[0m         _output_key\n\u001B[1;32m    458\u001B[0m     ]\n",
      "File \u001B[0;32m~/my_pyenv/AINote/lib/python3.10/site-packages/langchain/chains/base.py:258\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 258\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    259\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    260\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    261\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[1;32m    262\u001B[0m )\n",
      "File \u001B[0;32m~/my_pyenv/AINote/lib/python3.10/site-packages/langchain/chains/base.py:252\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[1;32m    246\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    247\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    248\u001B[0m     inputs,\n\u001B[1;32m    249\u001B[0m )\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 252\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    253\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    254\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    255\u001B[0m     )\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/my_pyenv/AINote/lib/python3.10/site-packages/langchain/chains/router/base.py:100\u001B[0m, in \u001B[0;36mMultiRouteChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_chain(route\u001B[38;5;241m.\u001B[39mnext_inputs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m route\u001B[38;5;241m.\u001B[39mdestination \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdestination_chains:\n\u001B[0;32m--> 100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdestination_chains\u001B[49m\u001B[43m[\u001B[49m\u001B[43mroute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdestination\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m        \u001B[49m\u001B[43mroute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msilent_errors:\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_chain(route\u001B[38;5;241m.\u001B[39mnext_inputs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n",
      "File \u001B[0;32m~/my_pyenv/AINote/lib/python3.10/site-packages/langchain/chains/base.py:235\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    202\u001B[0m     inputs: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Any],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    208\u001B[0m     include_run_info: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    209\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m    210\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    211\u001B[0m \n\u001B[1;32m    212\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03m            `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 235\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprep_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    236\u001B[0m     callback_manager \u001B[38;5;241m=\u001B[39m CallbackManager\u001B[38;5;241m.\u001B[39mconfigure(\n\u001B[1;32m    237\u001B[0m         callbacks,\n\u001B[1;32m    238\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    243\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata,\n\u001B[1;32m    244\u001B[0m     )\n\u001B[1;32m    245\u001B[0m     new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/my_pyenv/AINote/lib/python3.10/site-packages/langchain/chains/base.py:389\u001B[0m, in \u001B[0;36mChain.prep_inputs\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m     external_context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mload_memory_variables(inputs)\n\u001B[1;32m    388\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mexternal_context)\n\u001B[0;32m--> 389\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n",
      "File \u001B[0;32m~/my_pyenv/AINote/lib/python3.10/site-packages/langchain/chains/base.py:147\u001B[0m, in \u001B[0;36mChain._validate_inputs\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    145\u001B[0m missing_keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_keys)\u001B[38;5;241m.\u001B[39mdifference(inputs)\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m missing_keys:\n\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing some input keys: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmissing_keys\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Missing some input keys: {'subject'}"
     ]
    }
   ],
   "source": [
    "result = mult_chain.run({'input':'设计一个宇宙空间站的游戏场景'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T07:46:36.333429Z",
     "start_time": "2023-08-15T07:46:34.246597Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part-5：Memory\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part-6：Agent\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
