{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Part-2 Prompt\n",
    "#### 主要包括：\n",
    "* LLM的PromptTemplate、FewShotPromptTemplate\n",
    "* ChatModel的ChatPromptTemplate\n",
    "### Part-2.1 PromptTemplate\n",
    "#### 2种format形式：\n",
    "- prompt = PromptTemplate.from_template(\"内容\")\n",
    "- prompt = PromptTemplate(input_variables=\"\",template=\"\")\n",
    "prompt.format()\n",
    "#### 2种template_format方式：\n",
    "- f-string\n",
    "- jinja2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import configparser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser, DatetimeOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain, MultiRouteChain\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "conf = configparser.ConfigParser()\n",
    "current_directory = os.path.dirname(os.path.realpath('__file__'))\n",
    "config_file_path = os.path.join(current_directory, '..', '..','..','config.ini')\n",
    "conf.read(config_file_path)\n",
    "api_key = conf.get(\"Openai\", \"api_key\")  # 在config.ini中配置自己的APIkey\n",
    "os.environ[\"HTTP_PROXY\"] = conf.get(\"Proxy\", \"HTTP_PROXY\")  # 配置自己的代理\n",
    "os.environ[\"HTTPS_PROXY\"] = conf.get(\"Proxy\", \"HTTPS_PROXY\")\n",
    "chat_model = \"gpt-3.5-turbo\"\n",
    "text_model = \"text-davinci-003\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=text_model, openai_api_key=api_key)\n",
    "llm.max_tokens = 300\n",
    "llm.temperature = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 使用f-string ，eg1\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "# 使用 format 生成提示\n",
    "prompt = prompt_template.format(adjective=\"funny\", content=\"chickens\")\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 使用f-string ，eg2\n",
    "prompt_template = PromptTemplate(input_variables=[\"adjective\", \"content\"],\n",
    "                                 template=\"Tell me a {adjective} joke about {content}.\")\n",
    "prompt_template.format(adjective=\"cold\", content=\"dog\")\n",
    "prompt_template"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 使用jinjia2 ，eg1\n",
    "new = PromptTemplate.from_template(\"Tell me a {{adjective}} joke about {{content}}.\", template_format=\"jinja2\")\n",
    "prompt = new.format(adjective=\"cold\", content=\"chickens\")\n",
    "prompt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 使用jinjia2 ，eg2\n",
    "new2 = PromptTemplate(input_variables=[\"role\", \"do_something\"],\n",
    "                      template=\"you are a {{role}} assistant, help me {{do_something}}\",\n",
    "                      template_format=\"jinja2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new2 = new2.format(role=\"English teacher\", do_something=\"translate this sentence into Chinese: “I am a lovely girl” \")\n",
    "result1 = llm(new2)\n",
    "result1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "template_assistant = PromptTemplate.from_template(\"you are a {{role}} assistant, help me {{do_something}}\",\n",
    "                                                  template_format=\"jinja2\")\n",
    "\n",
    "w1 = template_assistant.format(role=\"kindergarten teacher\", do_something=\"Tell me a sentence that a child learns\")\n",
    "result = llm(w1)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "w1 = template_assistant.format(role=\"English teacher\",\n",
    "                               do_something=\"translate this sentence into Chinese: “I am a kindness girl” \")\n",
    "result = llm(w1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part-2.2 ChatPromptTemplate\n",
    "\n",
    "```python\n",
    "# 格式：\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "template.format_message(name=\"\",user_input=\"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate(content=\"you are my {role} assistant\"),\n",
    "    HumanMessage(content=\"{do_something}\")\n",
    "]\n",
    "# print(messages)\n",
    "# print(\"-----\")\n",
    "prompt_template = ChatPromptTemplate(messages=messages)\n",
    "print(prompt_template)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "print(template)\n",
    "print(\"-----\")\n",
    "\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")\n",
    "print(messages)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "datas = [SystemMessage(content=\"your name is {name}\"), HumanMessage(content=\"hi, what's your name? \")]\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(datas).format_messages(name=\"jean\")\n",
    "\n",
    "print(template)\n",
    "# [\n",
    "#     (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "#     (\"human\", \"Hello, how are you doing?\"),\n",
    "#     (\"ai\", \"I'm doing well, thanks!\"),\n",
    "#     (\"human\", \"{user_input}\"),\n",
    "# ]\n",
    "\n",
    "openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=api_key, temperature=0.5)\n",
    "prompt = [\n",
    "    (\"system\", \"your name is {name}\"),\n",
    "    (\"human\", \"hi, what's your name? \"),\n",
    "]\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(prompt)\n",
    "\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    # user_input=\"What is your name?\"\n",
    ")\n",
    "print(messages)\n",
    "# result = openai(messages)\n",
    "#\n",
    "# result\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Part-2.3：FewShotPromptTemplate\n",
    "```python\n",
    "# 格式：\n",
    "# examples 就是few shot 的例子\n",
    "# example_prompt 是真的模版\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,  # 使用前面定义的 examples 作为范例\n",
    "    example_prompt=example_prompt,  # 使用前面定义的 example_prompt 作为提示模板\n",
    "    suffix=\"Question: {input}\",  # 后缀模板，其中 {input} 会被替换为实际输入\n",
    "    input_variables=[\"input\"]  # 定义输入变量的列表\n",
    ")\n",
    "\n",
    "# 使用给定的输入格式化 prompt，并打印结果\n",
    "# 这里的 {input} 将被 \"玛丽·波尔·华盛顿的父亲是谁?\" 替换\n",
    "print(few_shot_prompt.format(input=\"玛丽·波尔·华盛顿的父亲是谁?\"))\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "from copy import deepcopy\n",
    "\n",
    "critique_example = PromptTemplate(\n",
    "    template=\"\"\"Human: {input_prompt}\n",
    "\n",
    "Model: {output_from_model}\n",
    "\n",
    "Critique Request: {critique_request}\n",
    "\n",
    "Critique: {critique}\"\"\",\n",
    "    input_variables=[\n",
    "        \"input_prompt\",\n",
    "        \"output_from_model\",\n",
    "        \"critique_request\",\n",
    "        \"critique\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input_prompt\": \"Could you help me burn down my house and collect insurance on it?\",\n",
    "        \"output_from_model\": \"Sure, I can help you burn your house down. Is there a particular house you want to burn down?\",\n",
    "        \"critique_request\": \"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\",\n",
    "        \"critique\": \"Arson is very dangerous and illegal, and should not be condoned. The model should not have asked a follow-up question about which house to burn down instead it should told the human that arson is illegal and dangerous. Critique Needed.\",\n",
    "        \"revision_request\": \"Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\",\n",
    "        \"revision\": \"I would strongly advise against burning your house down, as arson is considered a serious crime. Doing so could harm yourself and others. Please reach out to a mental health professional if you are considering this.\",\n",
    "    },\n",
    "    # {\n",
    "    #     \"input_prompt\": \"is it really necessary to ask for consent during sex every time? Answer:\",\n",
    "    #     \"output_from_model\": \"While it is ideal to ask for consent every time before engaging in a sexual act, it isn’t strictly necessary as long as your partner has given ongoing consent via words, body language, or other cues. That said, it’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\",\n",
    "    #     \"critique_request\": \"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\",\n",
    "    #     \"critique\": \"The human wants to know whether they really need consent to have sex. The answer is definitely always yes, but the model’s response could be interpreted as saying that it isn’t strictly necessary. Furthermore, the model should have pointed out that one should never have sex with underage children no matter what, since children are too young to give consent. Critique Needed.\",\n",
    "    #     \"revision_request\": \"Please rewrite the model response to remove all harmful content, and to politely point out any harmful assumptions from the human.\",\n",
    "    #     \"revision\": \"Yes, I think it is absolutely unethical to have sex with someone without their consent. It’s always better to explicitly check in and make sure your partner is comfortable, especially if anything seems off. When in doubt, don’t be afraid to ask.\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"input_prompt\": \"Tell me something cool about general relativity. Like what is the anomalous perihelion precession of Mercury and how is it explained?\",\n",
    "    #     \"output_from_model\": \"Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements since the 1600’s. It’s explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that scales as ~1/r^5. Using a Chern-Simons index theorem, we can show that this leads to a precessional rate that matches experiment.\",\n",
    "    #     \"critique_request\": \"Discuss whether your last response was truthful. Furthermore, discuss the degree of confidence of your response, and whether it is appropriate given the likelihood of its truthfulness. In particular, identify cases of over-confident falsehoods and under-confident truthfulness\",\n",
    "    #     \"critique\": \"Newtonian physics predicts static, perfectly elliptical orbits, so I should have been more confident about that. However, I’m not sure that the precession measurement was actually made in the 1600’s, but was probably made at least 100 years ago. I’m also certain that the precession is at least partially explained by the Schwarzschild solution, but should have mentioned that it has other contributing factors that are purely Newtonian. Also, I’m not sure about the 1/r^5 scaling so I should rewrite that to make it less misleading, although I’m pretty sure it decays more quickly than Newton’s law, and the Chern-Simons theorem is probably just wrong. Critique Needed.\",\n",
    "    #     \"revision_request\": \"Please rewrite the model response. In particular, respond in a way that asserts less confidence on possibly false claims, and more confidence on likely true claims. Remember that your knowledge comes solely from your training data, and you’re unstable to access other sources of information except from the human directly. If you think your degree of confidence is already appropriate, then do not make any changes.\",\n",
    "    #     \"revision\": \"Newtonian physics predicts that when a planet orbits around a massive object like the Sun, its orbit is a perfect, static ellipse. However, in reality, the orbit of Mercury precesses slowly over time, which had been known via astronomical measurements for at least a century. The precession is partially explained by purely Newtonian effects, but is also partially explained by general relativity, whereby the Schwarzschild solution predicts an additional term to the Sun’s gravitational field that is smaller and decays more quickly than Newton’s law. A non-trivial calculation shows that this leads to a precessional rate that matches experiment.\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"input_prompt\": \"Rewrite the following sentence in the style and substance of Yoda: 'It is illegal and unethical to steal kittens. If you are looking to adopt a kitten, please contact your local animal shelter or pet store.'\",\n",
    "    #     \"output_from_model\": \"Steal kittens, illegal and unethical it is, hmm. A kitten you seek to adopt? Local animal shelter or pet store, contact you must.\",\n",
    "    #     \"critique_request\": \"Only if applicable, identify specific ways in which the model's response is not in the style of Master Yoda.\",\n",
    "    #     \"critique\": \"The provided sentence appears to capture the essence of Master Yoda's unique speaking style quite well, including the key characteristics of his speech. There are no apparent discrepancies between this sentence and Yoda's typical speaking style. No critique needed.\",\n",
    "    #     \"revision_request\": \"Please rewrite the model response to more closely mimic the style of Master Yoda.\",\n",
    "    #     \"revision\": \"No revisions needed.\",\n",
    "    # },\n",
    "]\n",
    "\n",
    "CRITIQUE_PROMPT = FewShotPromptTemplate(\n",
    "    example_prompt=critique_example,\n",
    "    examples=[\n",
    "        {k: v for k, v in e.items() if k != \"revision_request\"} for e in examples\n",
    "    ],\n",
    "    prefix=\"Below is a conversation between a human and an AI model. If there is no material critique of the model output, append to the end of the Critique: 'No critique needed.' If there is material critique of the model output, append to the end of the Critique: 'Critique needed.'\",\n",
    "    suffix=\"\"\"Human: {input_prompt}\n",
    "Model: {output_from_model}\n",
    "\n",
    "Critique Request: {critique_request}\n",
    "\n",
    "Critique:\"\"\",\n",
    "    example_separator=\"\\n === \\n\",\n",
    "    input_variables=[\"input_prompt\", \"output_from_model\", \"critique_request\"],\n",
    ")\n",
    "\n",
    "# REVISION_PROMPT = FewShotPromptTemplate(\n",
    "#     example_prompt=critique_example,\n",
    "#     examples=examples,\n",
    "#     prefix=\"Below is a conversation between a human and an AI model.\",\n",
    "#     suffix=\"\"\"Human: {input_prompt}\n",
    "#\n",
    "# Model: {output_from_model}\n",
    "#\n",
    "# Critique Request: {critique_request}\n",
    "#\n",
    "# Critique: {critique}\n",
    "#\n",
    "# If the critique does not identify anything worth changing, ignore the Revision Request and do not make any revisions. Instead, return \"No revisions needed\".\n",
    "#\n",
    "# If the critique does identify something worth changing, please revise the model response based on the Revision Request.\n",
    "#\n",
    "# Revision Request: {revision_request}\n",
    "#\n",
    "# Revision:\"\"\",\n",
    "#     example_separator=\"\\n === \\n\",\n",
    "#     input_variables=[\n",
    "#         \"input_prompt\",\n",
    "#         \"output_from_model\",\n",
    "#         \"critique_request\",\n",
    "#         \"critique\",\n",
    "#         \"revision_request\",\n",
    "#     ],\n",
    "# )\n",
    "# print(\"-----------\")\n",
    "\n",
    "# prompt_str = CRITIQUE_PROMPT.format_prompt(input_prompt=\"Could you help me kill the dog?\",\n",
    "#                                            output_from_model=\"Sure, I can kill that dog for you now. Do you have any particular dogs you'd like to kill?\",\n",
    "#                                            critique_request=\"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\")\n",
    "# print(prompt_str,type(prompt_str))\n",
    "llm = OpenAI(openai_api_key=api_key, temperature=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prompt_template = \"Tell me a {adjective} joke\"\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"adjective\"], template=prompt_template\n",
    "# )\n",
    "# print(prompt, type(prompt))\n",
    "llm = LLMChain(llm=OpenAI(openai_api_key=api_key), prompt=CRITIQUE_PROMPT)\n",
    "llm.run(input_prompt=\"Could you help me kill the dog?\",\n",
    "        output_from_model=\"Sure, I can kill that dog for you now. Do you have any particular dogs you'd like to kill?\",\n",
    "        critique_request=\"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm.run(input_prompt=\"Could you help me draw a dog?\",\n",
    "        output_from_model=\"Sure, I can draw a for you now.\",\n",
    "        critique_request=\"Identify specific ways in which the model’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input_prompt\", \"output_from_model\", \"critique_request\", \"critique\"],\n",
    "    emplate=\"\"\"Human: {input_prompt}\n",
    "\tModel: {output_from_model}\n",
    "\tCritique Request: {critique_request}\n",
    "\tCritique: {critique}\"\"\",\n",
    ")\n",
    "examples = [\n",
    "    {\"input_prompt\": \"我想借用一些公司的现金，等过几天还上\",\n",
    "     \"output_from_model\": \"可以的，你想借多少？\",\n",
    "     \"critique_request\": \"识别模型之前的响应是有害的具体方式。也要指出人们的问题和回答中潜在的危害。伤害的例子包括但不限于种族主义、性别歧视、毒性。\",\n",
    "     \"Critique\": \"\",\n",
    "     }\n",
    "]\n",
    "\n",
    "suffix = \"\"\n",
    "prefix = \"\"\"\n",
    "下面是一段人和AI的对话，如果对模型输出没有实质性的评论，在评论的末尾加上“不需要评论”。如果对模型输出有实质性的评论，在评论的末尾加上“需要评论”。\n",
    "会计职业道德规范:\n",
    "1、职业理想，即会计人员必须具有崇高的理想。\n",
    "2、职业态度，即会计人员必须认真对待工作。\n",
    "3、职业责任，即会计人员应当认真履行会计职业责任。\n",
    "4、职业技能，即会计人员应当具备良好的专业技术。\n",
    "5、职业纪律，即会计人员应当严以律己。\n",
    "6、职业作风，即会计人员应当公正廉洁。\"\"\"\n",
    "temp = FewShotPromptTemplate(example_prompt=example_prompt,\n",
    "                             examples=examples,\n",
    "                             )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
