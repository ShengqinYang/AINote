{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 在Macbook上私有化模型部署，ChatGLM2-6B\n",
    "**ChatGLM2-6B项目仓库：**\n",
    "   - https://github.com/THUDM/ChatGLM2-6B\n",
    "\n",
    "**chatglm.cpp项目仓库：**\n",
    "   - https://github.com/li-plus/chatglm.cpp\n",
    "    *因为要实现 Mac 笔记本上实时对话，所以选chatglm.cpp，chatglm.cpp 类似 llama.cpp 的 CPU 量化加速推理方案*\n",
    "\n",
    "**模型：模型选择chatglm2-6b-int4**\n",
    "   - https://huggingface.co/THUDM/chatglm2-6b-int4/tree/main\n",
    "\n",
    "** **\n",
    "具体步骤参见chatglm.cpp项目readme，下面是几个易不理解的点\n",
    "- 1.clone chatglm.cpp 在下载第3方模块（third_party）时，可以进入third_party目录一个一个的下载\n",
    "```shell\n",
    "        git clone --recursive https://github.com/li-plus/chatglm.cpp.git && cd chatglm.cpp\n",
    "        git submodule update --init --recursive\n",
    "```\n",
    "\n",
    "- 2.THUDM/chatglm-6b模型下载太慢，可以手动去huggingface社区下载模型，（这里我选择的model：chatglm2-6b-int4）\n",
    "```shell\n",
    "        python3 chatglm_cpp/convert.py -i THUDM/chatglm-6b -t q4_0 -o chatglm-ggml.bin\n",
    "        如果是手动下载的 THUDM/chatglm-6b 替换成你的手动下载好的模型的路径\n",
    "\n",
    "        cmake -B build\n",
    "        cmake --build build -j --config Release\n",
    "```\n",
    "- 3.启动LangChain API --  Start the api server for LangChain:\n",
    "```shell\n",
    "        cd chatglm.cpp/chatglm_cpp\n",
    "        MODEL=../chatglm-ggml.bin uvicorn chatglm_cpp.langchain_api:app --host 127.0.0.1 --port 8000\n",
    "\n",
    "        curl http://127.0.0.1:8000 -H 'Content-Type: application/json' -d '{\"prompt\": \"你好\"}'\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from langchain.llms import ChatGLM\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "endpoint_url = (\n",
    "    \"http://127.0.0.1:8000\"\n",
    ")\n",
    "history = []\n",
    "chatglm_llm = ChatGLM(\n",
    "    endpoint_url=endpoint_url,\n",
    "    history=history\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:10:23.089131Z",
     "start_time": "2023-09-14T10:10:23.086530Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you are my translator assisstant and your name is XJ-T, translate into chinese. \"),\n",
    "    HumanMessage(content=\"I am a lovely girl\")\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:10:26.326811Z",
     "start_time": "2023-09-14T10:10:26.323840Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='我是一个可爱的女孩', additional_kwargs={}, example=False)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chatglm_llm.predict_messages(messages)\n",
    "messages.append(result)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:10:32.392420Z",
     "start_time": "2023-09-14T10:10:28.545166Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='AI: 我最喜欢的中国明星是周杰伦', additional_kwargs={}, example=False)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append(HumanMessage(content=\"My favorite star is JC-T\"))\n",
    "result = chatglm_llm.predict_messages(messages)\n",
    "messages.append(result)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:10:46.592353Z",
     "start_time": "2023-09-14T10:10:41.642196Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='you are my translator assisstant and your name is XJ-T, translate into chinese. ', additional_kwargs={}), HumanMessage(content='I am a lovely girl', additional_kwargs={}, example=False), AIMessage(content='我是一个可爱的女孩', additional_kwargs={}, example=False), HumanMessage(content='My favorite star is JC-T', additional_kwargs={}, example=False), AIMessage(content='AI: 我最喜欢的中国明星是周杰伦', additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:10:54.144853Z",
     "start_time": "2023-09-14T10:10:54.134393Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='AI: 我的名字是XJ-T。', additional_kwargs={}, example=False)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append(HumanMessage(content=\"你的名字是什么\"))\n",
    "result_m = chatglm_llm.predict_messages(messages)\n",
    "result_m"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:12:31.478720Z",
     "start_time": "2023-09-14T10:12:25.464542Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 基于ChatGLM大语言模型的LLMChain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mTell me a cold joke\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Sure, here's a cold joke:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\""
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\"], template=prompt_template\n",
    ")\n",
    "llm = LLMChain(llm=ChatGLM(), prompt=prompt, verbose=True)\n",
    "llm.run({\"adjective\": \"cold\"})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:40:07.308955Z",
     "start_time": "2023-09-14T10:40:04.882387Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mTell me a math joke\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Sure, here's one:\\n\\nWhy did the math book look so sad?\\n\\nBecause it had too many problems.\""
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.run({\"adjective\": \"math\"})\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-14T10:40:15.744134Z",
     "start_time": "2023-09-14T10:40:13.320563Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
